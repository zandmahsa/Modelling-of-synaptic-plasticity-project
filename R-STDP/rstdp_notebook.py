# -*- coding: utf-8 -*-
"""RSTDP-notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169gtlcaguY56s0SFBY6xxcvKhHFT_Gse
"""

# Download N-MNIST dataset from Google Drive
!rm -f Train.zip Test.zip LICENSE.txt ReadMe\(MNIST\).txt
!gdown '1f-GwPB4pv0vqd0BqVs4_NsVYB4UTjsc-' -O Train.zip
!gdown '1HAyMnpJdF_kxQEiO64D3P0ygImAfeg9f' -O Test.zip
!unzip -oq Train.zip -d nmnist
!unzip -oq Test.zip -d nmnist

!pip install git+https://github.com/BindsNET/bindsnet.git@59d662a -q

# Add here necessary libraries
import os
import numpy as np
import torch
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
from tqdm import tqdm
import gc
import gym
from gym import spaces

# Bindsnet needs to be installed
from bindsnet.network import Network
from bindsnet.network.nodes import Input, LIFNodes
from bindsnet.network.topology import Connection
from bindsnet.network.monitors import Monitor
from bindsnet.learning import MSTDP

# Some general parameters (change as needed)
n_neurons = 10
time = 300  # Number of timesteps per sample
n_train = 2000  # Number of training samples
n_test = 200 # Number of test samples
update_interval = 200  # Number of steps between weight updates
n_epochs = 10

cumulative = True
separate_polarities = False
flatten = True
random = False

class NMNISTDataset(Dataset):
    def __init__(self, root, train=True, transform=None, num_samples=None, time=None,
                flatten=False, separate_polarities=False, cumulative=False, lazy=False, random=False,
                 update_interval=300):
        self.root = root
        self.train = train
        self.transform = transform
        self.flatten = flatten
        self.separate_polarities = separate_polarities
        self.cumulative = cumulative
        self.lazy = lazy
        self.time = time
        self.data, self.labels = self.load_data(num_samples=num_samples, update_interval=update_interval)
        if random:
            self.idx_order = np.random.permutation(len(self.data))
        else:
            self.idx_order = np.arange(len(self.data))

    def shuffle(self, on=True):
        if on:
            self.idx_order = np.random.permutation(len(self.data))
        else:
            self.idx_order = np.arange(len(self.data))


    def load_data(self, num_samples=None, update_interval=300):
        if self.train:
            data_path = os.path.join(self.root, 'Train')
        else:
            data_path = os.path.join(self.root, 'Test')

        data = []
        labels = []
        max_time = 320000

        for label in range(10):
            label_path = os.path.join(data_path, str(label))
            if self.train:
                print(f"Loading training data for label {label} from {label_path}")
            else:
                print(f"Loading testing data for label {label} from {label_path}")

            if num_samples and (num_samples//10 < len(os.listdir(label_path))):
                data_files = np.random.choice(os.listdir(label_path), num_samples//10, replace=False)
            else:
                data_files = os.listdir(label_path)


            for i, file in enumerate(tqdm(data_files)):
                if (i+1) % update_interval == 0:
                    print(f"Loaded {i+1}/{len(data_files)} files for label {label}")
                labels.append(label)
                file_path = os.path.join(label_path, file)
                file_data = np.fromfile(file_path, dtype=np.uint8)
                if self.lazy:
                    data.append(file_data)
                else:
                    file_data = file_data.reshape(-1, 5)
                    event_data = np.apply_along_axis(self._bytes_to_event, 1, file_data)
                    spikes = self._events_to_spikes(event_data, max_time)
                    data.append(spikes)
                    del spikes
                    del event_data

                del file_data
                gc.collect()

        return data, labels

    def _events_to_spikes(self, events, max_time = 320000):
        if self.time is not None:
            spikes = torch.zeros((self.time, 2 if self.separate_polarities else 1, 34, 34))
            time_step = np.ceil(max_time / self.time)
        else:
            spikes = torch.zeros((max_time, 2 if self.separate_polarities else 1, 34, 34))

        for event in events:
            x = event[0]
            y = event[1]
            p = event[2]
            t = event[3]
            if self.time is not None:
                t = int(t / time_step)
            if self.separate_polarities:
                if self.cumulative:
                    spikes[t:, 1 if p else 0, y, x] = 1
                else:
                    spikes[t, 1 if p else 0, y, x] = 1
            else:
                if self.cumulative:
                    spikes[t:, 0, y, x] += (p - 0.5) * 2
                else:
                    spikes[t, 0, y, x] += (p - 0.5) * 2

        if self.flatten:
            spikes = spikes.view(self.time, *([2, -1] if self.separate_polarities else [1, -1]))

        return spikes



    def _bytes_to_event(self, bytes_data):
        bits = bin(int.from_bytes(bytes_data, byteorder='big'))[2:]
        bits = '0'*(40-len(bits)) + bits
        x = int(bits[:8],2)
        y = int(bits[8:16],2)
        p = int(bits[16:17],2)
        t = int(bits[17:],2)
        event = np.array((np.uint8(x), np.uint8(y), np.bool_(p), np.uint32(t)))
        return event

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if self.lazy:
            file_data = self.data[self.idx_order[idx]].reshape(-1, 5)
            event_data = np.apply_along_axis(self._bytes_to_event, 1, file_data)
            sample = self._events_to_spikes(event_data)
        else:
            sample = self.data[self.idx_order[idx]]
        label = self.labels[self.idx_order[idx]]

        if self.transform:
            sample = self.transform(sample)

        return {'spikes': sample, 'label': label}

train_dataset_sample = NMNISTDataset(root='nmnist', train=True, num_samples=n_test, time=time, cumulative=cumulative,
                              flatten=flatten, separate_polarities=separate_polarities, lazy=False, random=random, update_interval=30)

def select_samples_by_label(dataset, num_samples_per_label=1):
    label_samples = {i: [] for i in range(10)}
    for i in range(len(dataset)):
        sample = dataset[i]
        label = sample['label']
        if len(label_samples[label]) < num_samples_per_label:
            label_samples[label].append(sample)
        if all(len(label_samples[l]) >= num_samples_per_label for l in range(10)):
            break
    return label_samples

def plot_samples(label_samples, flatten=False, separate_polarities=False, cumulative=False):
    if separate_polarities:
        fig, axes = plt.subplots(3, len(label_samples.items()), figsize=(20, 6))
    else:
        fig, axes = plt.subplots(1, len(label_samples.items()), figsize=(20, 6))
    for label, samples in label_samples.items():
        for i, sample in enumerate(samples):
            if cumulative:
                if separate_polarities:
                    img_exh = sample['spikes'][-1][1].numpy()
                    img_inh = sample['spikes'][-1][0].numpy()
                else:
                    img = sample['spikes'][-1].numpy()
            else:
                spikes = sample['spikes']
                if separate_polarities:
                    img_exh = np.zeros_like(spikes[0,1].numpy())
                    img_inh = np.zeros_like(spikes[0,0].numpy())
                    for t in range(spikes.shape[0]):
                        img_exh+=spikes[t,1].numpy()
                        img_inh+=spikes[t,0].numpy()
                else:
                    img = np.zeros_like(spikes[0].numpy())
                    for t in range(spikes.shape[0]):
                        img+=spikes[t].numpy()
            if flatten:
                if separate_polarities:
                    img_exh = img_exh.reshape(34, 34)
                    img_inh = img_inh.reshape(34, 34)
                else:
                    img = img.reshape(34, 34)
            if separate_polarities:
                axes[0, label].imshow(img_inh, cmap='viridis', interpolation='nearest')
                axes[0, label].set_title(f'Label: {label} (Inh)')
                axes[0, label].axis('off')
                axes[1, label].imshow(img_exh, cmap='viridis', interpolation='nearest')
                axes[1, label].set_title(f'Label: {label} (exh)')
                axes[1, label].axis('off')
                axes[2, label].imshow(img_exh-img_inh, cmap='viridis', interpolation='nearest')
                axes[2, label].set_title(f'Label: {label} (exh-Inh)')
                axes[2, label].axis('off')
            else:
                axes[label].imshow(img, cmap='viridis', interpolation='nearest')
                axes[label].set_title(f'Label: {label}')
                axes[label].axis('off')
    plt.show()

def plot_one_sample(sample, time, separate_polarities=False, cumulative=False):
    spikes = sample['spikes']
    if separate_polarities:
        fig_1, ax_1 = plt.subplots(time//20,20, figsize=(20,time//20))
        fig_2, ax_2 = plt.subplots(time//20,20, figsize=(20,time//20))
    else:
        fig_1, ax_1 = plt.subplots(time//20,20, figsize=(20,time//20))

    if not cumulative:
        if separate_polarities:
            current_sum = np.zeros((2,34,34))
        else:
            current_sum = np.zeros((34,34))

    for i in range(time//20):
        for j in range(20):
              if not cumulative:
                  if separate_polarities:
                      current_sum += spikes[i*(time//20)+j].numpy().reshape(2,34,34)
                      ax_1[i,j].imshow(current_sum[0], cmap='viridis', interpolation='nearest')
                      ax_1[i,j].axis('off')
                      ax_2[i,j].imshow(current_sum[1], cmap='viridis', interpolation='nearest')
                      ax_2[i,j].axis('off')
                  else:
                      current_sum += spikes[i*(time//20)+j].numpy().reshape(34,34)
                      ax_1[i,j].imshow(current_sum, cmap='viridis', interpolation='nearest')
                      ax_1[i,j].axis('off')
              else:
                  if separate_polarities:
                      ax_1[i,j].imshow(spikes[i*(time//20)+j].numpy().reshape(2,34,34)[0], cmap='viridis', interpolation='nearest')
                      ax_1[i,j].axis('off')
                      ax_2[i,j].imshow(spikes[i*(time//20)+j].numpy().reshape(2,34,34)[1], cmap='viridis', interpolation='nearest')
                      ax_2[i,j].axis('off')
                  else:
                      ax_1[i,j].imshow(spikes[i*(time//20)+j].numpy().reshape(34,34), cmap='viridis', interpolation='nearest')
                      ax_1[i,j].axis('off')

    plt.show()

plot_one_sample(train_dataset_sample[np.random.randint(60)], time, separate_polarities=separate_polarities, cumulative=cumulative)

print("Training dataset samples:")
train_label_samples = select_samples_by_label(train_dataset_sample)
plot_samples(train_label_samples, flatten=flatten, separate_polarities=separate_polarities, cumulative=cumulative)

# Load N-MNIST dataset
train_dataset = NMNISTDataset(root='nmnist', train=True, num_samples=n_train, time=time, cumulative=cumulative,
                              flatten=flatten, separate_polarities=separate_polarities, lazy=False, random=random, update_interval=50)

test_dataset = NMNISTDataset(root='nmnist', train=False, num_samples=n_test, time=time, cumulative=cumulative,
                              flatten=flatten, separate_polarities=separate_polarities, lazy=False, random=random, update_interval=50)

# Check the loaded data
def check_data(dataset):
    label_counts = {i: 0 for i in range(10)}
    for i in range(len(dataset)):
        sample = dataset[i]
        label = sample['label']
        label_counts[label] += 1
    print("Label distribution:", label_counts)


print("Checking training dataset:")
check_data(train_dataset)

print("Checking testing dataset:")
check_data(test_dataset)

def softmax(x):
    res = np.exp(x)
    return res / res.sum()

class TimeEncodedMNISTEnv(gym.Env):
    def __init__(self, dataset, random=True):
        super(TimeEncodedMNISTEnv, self).__init__()
        self.dataset = dataset
        self.current_index = 0
        self.current_time_step = 0
        self.last_index = len(dataset) - 1
        self.current_prediction = None
        self.reset(random)

        self.last_time_step = dataset[0]['spikes'].shape[0] - 1

        self.observation_space = spaces.Box(low=0, high=1, shape=dataset[0]['spikes'][0].shape, dtype=np.uint8)

        # Action space represents the digits 0-9
        self.action_space = spaces.Box(low=0, high=1, shape=(1, 10), dtype=np.uint8)

    def reset(self, random=True):
        # Reset the index and get a new sample
        self.dataset.shuffle(random)
        self.current_index = 0
        self.current_time_step = 0
        self.current_prediction = None
        self.current_sample = self.dataset[self.current_index]['spikes']
        self.current_label = self.dataset[self.current_index]['label']

        # Return the initial observation
        return self.current_sample[self.current_time_step].reshape(1, *self.current_sample[self.current_time_step].shape)

    def next_sample(self):
        self.current_time_step = 0
        self.current_prediction = None
        self.current_index += 1
        self.current_sample = self.dataset[self.current_index]['spikes']
        self.current_label = self.dataset[self.current_index]['label']
        return self.current_sample[0].reshape(1, *self.current_sample[0].shape)

    def step(self, action):
        """
        action: A (1, 10) tensor representing the network's output spike train.
        """
        if self.current_prediction is None:
            self.current_prediction = action
        else:
            self.current_prediction = np.concatenate((self.current_prediction, action), axis=0)

        sum_actions = softmax(np.sum(self.current_prediction, axis=0))
        spike_percentage = np.mean(self.current_prediction)*100

        prediction = np.argwhere(sum_actions == np.max(sum_actions)).flatten().tolist()

        #accuracy = np.max(sum_actions) if self.current_label in prediction else 0
        accuracy = int(self.current_label in prediction)/len(prediction)

        label = self.current_label

        #reward = 0

        #reward = -1 * np.sum(action)
        #if action[0, self.current_label] == 1:
        #    reward += 6
        #reward = reward / 5

        #new reward

        reward = softmax(action.reshape(-1))[self.current_label] * 10 - 1

        ###########

        if self.current_index == self.last_index and self.current_time_step == self.last_time_step:
            done = True
        elif self.current_time_step == self.last_time_step:
            sample = self.next_sample()
            done = False
        else:
            done = False
            self.current_time_step += 1

        # Info dictionary (optional, can be used for debugging)
        info = {'label': label, 'prediction': prediction, 'accuracy': accuracy, 'spike_percentage': spike_percentage}

        observation = self.current_sample[self.current_time_step].reshape(1, *self.current_sample[self.current_time_step].shape)

        # Return the observation, reward, done flag, and info dictionary
        return observation, reward, done, info

    def render(self, mode='human'):
        pass

# Example usage with the dataset
# Assuming training_data is already defined as described
env = TimeEncodedMNISTEnv(train_dataset, random=random)
env_test = TimeEncodedMNISTEnv(test_dataset, random=random)

# Reset environment to get the first observation
observation = env.reset(random)

# Example loop for interacting with the environment
for _ in range(10):

    # Choose an action (for example, randomly)
    action = env.action_space.sample()

    # Step through the environment
    observation, reward, done, info = env.step(action)

    print(f"Action: {action}, reward: {reward}, prediction: {info['prediction']}, label: {info['label']}, accuracy: {info['accuracy']}, spike percentage: {info['spike_percentage']}")

    if done:
        # If the episode ends, reset the environment
        observation = env.reset(random)

# Close the environment
env.reset(random)

recurrent = False
random = True
train_dataset.shuffle(random)
gamma = 0.95
update_interval = 1000
n_epochs = 10
weight_decay = 0
lr_input = (1e-4, 1e-2)
lr_recurrent = (1e-2, 1e-2)
input_init_weight = 0.05 + 0.1*torch.randn(34*34, n_neurons)
recurrent_init_weight = 0.025*(torch.eye(n_neurons)-1)
input_scale_factor = 1

from sklearn.metrics import f1_score


def test_network(network, test_env, output_monitor, verbose =False):
    network = network.train(False)
    observation = env_test.reset(random)
    reward = 0
    labels = []
    predictions = []

    test_accuracy = np.zeros(n_test)
    for i in range(n_test):
        output_spikes = []
        for t in range(time):
            if separate_polarities:
                inputs = {'input_p_1': observation[:,1,:], 'input_p_0': observation[:,0,:]}
            else:
                inputs = {'input': observation}
            network.run(inputs=inputs, time=1, reward=reward, one_step=True)
            action = np.array(output_monitor.get('s').view(1, -1))
            output_spikes.append(action.reshape(-1))

            observation, reward, test_done, test_info = env_test.step(action)
        test_accuracy[i] = test_info['accuracy']

        labels.append(test_info['label'])
        predictions.append(test_info['prediction'])
        if verbose:
            print(f"Accuracy: {test_info['accuracy']}, Label: {test_info['label']}, Prediction: {test_info['prediction']}")
            plt.matshow(np.array(output_spikes).T)
            plt.show()

        network.reset_state_variables()
    network = network.train(True)


    # Convert the labels and predictions into binary format (one-hot encoding)
    def to_binary_matrix(labels, n_neurons):
        binary_matrix = np.zeros((len(labels), n_neurons), dtype=int)
        for i, label in enumerate(labels):
            binary_matrix[i, label] = 1
        return binary_matrix

    # Convert both true labels and predicted labels to binary matrices
    y_true_binary = to_binary_matrix(labels, n_neurons)
    y_pred_binary = np.zeros((len(predictions), n_neurons), dtype=int)
    for i, preds in enumerate(predictions):
        for pred in preds:
            y_pred_binary[i, pred] = 1

    # Calculate F1 score using 'micro', 'macro', or 'weighted' average for multi-label
    f1_micro = f1_score(y_true_binary, y_pred_binary, average='micro')
    f1_macro = f1_score(y_true_binary, y_pred_binary, average='macro')
    f1_weighted = f1_score(y_true_binary, y_pred_binary, average='weighted')

    # Output the F1 scores
    print("F1 Score (Micro):", f1_micro)
    print("F1 Score (Macro):", f1_macro)
    print("F1 Score (Weighted):", f1_weighted)

    return test_accuracy.mean()

network = Network(learning=True)
network.learning = True

if separate_polarities:
    if flatten:
        input_p_1 = Input(n=34*34, shape=(1, 34 * 34), traces=True)
        input_p_0 = Input(n=34*34, shape=(1, 34 * 34), traces=True)
    else:
        input_p_1 = Input(n=34*34, shape=(1, 34, 34), traces=True)
        input_p_0 = Input(n=34*34, shape=(1, 34, 34), traces=True)
    network.add_layer(input_p_1, name='input_p_1')
    network.add_layer(input_p_0, name='input_p_0')
else:
    if flatten:
        input_layer = Input(n=34*34, shape=(1, 34 * 34), traces=True)
    else:
        input_layer = Input(n=34*34, shape=(1, 34, 34), traces=True)
    network.add_layer(input_layer, name='input')
output_layer = LIFNodes(n=n_neurons, traces=True)
network.add_layer(output_layer, name='output')


if separate_polarities:
    connection_p_1 = Connection(source=input_p_1,
                                target=output_layer,
                                w=input_init_weight,
                                )
    update_rule_p_1 = MSTDP(connection_p_1, nu=lr_input, weight_decay=weight_decay)
    connection_p_0 = Connection(source=input_p_0,
                                target=output_layer,
                                  w=input_init_weight,
                                  )
    update_rule_p_0 = MSTDP(connection_p_0, nu=lr_input, weight_decay=weight_decay)
    connection_p_1.update_rule = update_rule_p_1
    connection_p_0.update_rule = update_rule_p_0
    network.add_connection(connection_p_1, source='input_p_1', target='output')
    network.add_connection(connection_p_0, source='input_p_0', target='output')
else:
    connection = Connection(source=input_layer,
                             target=output_layer,
                               w=input_init_weight,
                               )
    update_rule = MSTDP(connection, nu=lr_input, weight_decay=weight_decay)
    connection.update_rule = update_rule
    network.add_connection(connection, source='input', target='output')


if recurrent:
    connection_recurrent = Connection(source=output_layer, target=output_layer,
                                       w=recurrent_init_weight
                                       )
    update_rule_recurrent = MSTDP(connection_recurrent, nu=lr_recurrent, weight_decay=weight_decay)
    connection_recurrent.update_rule = update_rule_recurrent
    network.add_connection(connection_recurrent, source='output', target='output')





output_monitor = Monitor(obj=output_layer, state_vars=('s', 'v'), time=1)
network.add_monitor(output_monitor, name='output_spikes')

#start
observation = env.reset(random)

#set the initial reward


accuracy_record = np.zeros(n_epochs)
test_accuracy_record = np.zeros(n_epochs)

rand = random

network.train()
for epoch in range(n_epochs):
    spike_percentage = np.zeros(n_train)
    for i in (pbar := tqdm(range(n_train))):
        reward = 0
        for t in range(time):
            if separate_polarities:
                inputs = {'input_p_1': input_scale_factor * observation[:,1,:], 'input_p_0': input_scale_factor *observation[:,0,:]}
            else:
                inputs = {'input': input_scale_factor * observation}
            network.run(inputs=inputs, time=1, reward=input_scale_factor *reward)
            action = np.array(output_monitor.get('s').view(1, -1))

            observation, reward, done, info = env.step(action)

            spike_percentage[i] = info['spike_percentage']


            if (i+1) % update_interval == 0 and i!=0 and t == time-1:
                # Test the network
                test_accuracy = test_network(network, env_test, output_monitor)
                test_accuracy_record[epoch] = ((test_accuracy_record[epoch] * i) + test_accuracy) / (i + 1)

                pbar.set_description(f"Epoch: {epoch}, Accuracy: {round(accuracy_record[epoch] * 100, 2)}%, Test accuracy: {round(test_accuracy * 100,2)}%, Spike percentage: {round(spike_percentage[:i+1].mean(),2)}%")


                #####################


                fig, ax = plt.subplots(2 if separate_polarities else 1, 10, figsize=(10, 5 if separate_polarities else 2))

                for j in range(10):
                    if separate_polarities:
                        ax[0,j].imshow(connection_p_1.w.view(-1, n_neurons).detach().numpy().T[j].reshape(34,34), cmap='viridis')
                        ax[0,j].axis('off')
                        ax[1,j].imshow(connection_p_0.w.view(-1, n_neurons).detach().numpy().T[j].reshape(34,34), cmap='viridis')
                        ax[1,j].axis('off')
                    else:
                        ax[j].imshow(connection.w.view(-1, n_neurons).detach().numpy().T[j].reshape(34,34), cmap='viridis')
                        ax[j].axis('off')

                plt.show()



            if done:
                observation = env.reset(rand)
        accuracy_record[epoch] = ((accuracy_record[epoch] * i) + info['accuracy']) / (i + 1)
        if (i+1)%10==0:
            pbar.set_description(f"Epoch: {epoch}, Accuracy: {round(accuracy_record[epoch] * 100,2)}%, Spike percentage: {round(spike_percentage[:i+1].mean(),2)}%")
        network.reset_state_variables()

    if separate_polarities:
        update_rule_p_1.nu *= gamma
        update_rule_p_0.nu *= gamma
    else:
        update_rule.nu *= gamma

    if recurrent:
        update_rule_recurrent.nu *= gamma

plt.plot(test_accuracy_record)
plt.show()

test_accuracy_record = np.array([0.1884, 0.1693, 0.1758, 0.1621, 0.1732, 0.1815, 0.1807, 0.1907, 0.1848, 0.1832])
plt.plot(test_accuracy_record)
plt.show()
plt.plot(accuracy_record)
plt.show()

env_test = TimeEncodedMNISTEnv(test_dataset)

print('Average accuracy: ',test_network(network, env_test, output_monitor, verbose=True))

